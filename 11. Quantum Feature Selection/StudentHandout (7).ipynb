{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Quantum Feature Selection — Detailed Notes (Session 11)\n",
        "**Course:** CS490/5590 — Quantum Computing Applications in Data Science, AI, & Deep Learning  \n",
        "**Instructor:** Luke Miller  \n",
        "\n",
        "> **Purpose.**  These notes expand the slide bullets into a stand-alone reference. You’ll learn what “feature selection” means in the quantum-kernel setting, how to approximate quantum mutual information, how to build hybrid quantum-classical pipelines that choose a small but informative subset of features, and how to prototype everything in Qiskit Machine Learning.  Mini-exercises (with brief answers) are included at the end.\n",
        "\n",
        "---\n",
        "\n",
        "## Session Road-map  \n",
        "1. Recap: quantum kernels & QSVMs  \n",
        "2. Why feature selection? challenges in high-dimensional QML  \n",
        "3. Quantum mutual information (QMI) as a relevance score  \n",
        "4. Quantum-kernel–based scoring (individual & joint)  \n",
        "5. Circuits for QMI / kernel estimation (swap test, overlap)  \n",
        "6. Hybrid pipeline: quantum scoring → classical ranking  \n",
        "7. Qiskit demo (breast-cancer dataset, top-k selection)  \n",
        "8. Noise analysis & mitigation strategies  \n",
        "9. Q&A  \n",
        "\n",
        "---\n",
        "\n",
        "## 0) Why bother with feature selection in quantum ML?  \n",
        "\n",
        "| Pain-point | Consequence | Feature-selection remedy |\n",
        "|------------|-------------|--------------------------|\n",
        "| # qubits = # selected features | Chip limit (≤ 127 today) | Drop irrelevant features |\n",
        "| Kernel matrix $K_{ij}$ cost $O(N^2)$ circuits | Expensive for large $N$ or shots | Smaller feature set  → shorter circuits |\n",
        "| Noise grows with circuit depth | Kernel / MI estimates biased | Fewer qubits → shallower entangling layers |\n",
        "\n",
        "Goal: **retain predictive power** while respecting NISQ resource bounds.\n",
        "\n",
        "---\n",
        "\n",
        "## 1) Classical vs quantum mutual information  \n",
        "\n",
        "### 1.1 Classical MI  \n",
        "$$\n",
        "I(X;Y)=\\sum_{x,y}p_{XY}(x,y)\\log\\frac{p_{XY}(x,y)}{p_X(x)p_Y(y)}.\n",
        "$$\n",
        "\n",
        "### 1.2 Quantum MI  \n",
        "Given bipartite state $\\rho_{XY}$,\n",
        "$$\n",
        "I_Q(X{:}Y)=S(\\rho_X)+S(\\rho_Y)-S(\\rho_{XY}),\n",
        "$$\n",
        "where $S(\\rho)=-\\operatorname{Tr}(\\rho\\log\\rho)$ is von Neumann entropy.\n",
        "\n",
        "- Captures *classical* **and** *entanglement* correlations.  \n",
        "- Reduces to classical MI when $\\rho_{XY}$ is classical mixture.\n",
        "\n",
        "---\n",
        "\n",
        "## 2) Encoding features into quantum states  \n",
        "\n",
        "- **Assign one qubit per feature** (or encode block of features).  \n",
        "- **Feature map** $U_\\phi(x)$ (e.g., ZZFeatureMap depth = 1–2).  \n",
        "- For *single-feature relevance* compute state of qubit $i$ conditioned on label.\n",
        "\n",
        "**Binary label encoding**: extra ancilla qubit $|y\\rangle$ or classical conditioning.\n",
        "\n",
        "---\n",
        "\n",
        "## 3) Estimating quantum mutual information  \n",
        "\n",
        "### 3.1 Entropy via swap test (two copies)  \n",
        "\n",
        "$$\n",
        "S_2(\\rho)= -\\log \\operatorname{Tr}(\\rho^2)\n",
        "\\;\\;\\text{(Rényi-2 entropy, cheap to measure).}\n",
        "$$\n",
        "\n",
        "Swap-test circuit measures $\\operatorname{Tr}(\\rho^2)$. Use as proxy for von Neumann entropy.\n",
        "\n",
        "### 3.2 Workflow for feature $f_i$\n",
        "\n",
        "1. Prepare dataset superposition  \n",
        "   $|\\Psi\\rangle=\\frac1{\\sqrt N}\\sum_{k}|x_{k,i}\\rangle\\,|y_k\\rangle$.  \n",
        "2. Trace out all but qubit $i$ (feature) and label qubit $\\ell$.  \n",
        "3. Estimate swap test on $\\rho_i$, $\\rho_\\ell$, and joint $\\rho_{i\\ell}$.  \n",
        "4. Compute\n",
        "   $I_Q(f_i{:}Y)=S_2(\\rho_i)+S_2(\\rho_\\ell)-S_2(\\rho_{i\\ell}).$\n",
        "\n",
        "Rank features by $I_Q$.\n",
        "\n",
        "> **Shot count** scales with number of qubits measured, not with dataset size: quantum parallelism!\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Kernel-based scoring alternative  \n",
        "\n",
        "- Compute quantum kernel matrix **with and without** feature $f_i$.  \n",
        "- **Score** $s_i = \\text{Accuracy}_\\text{all} - \\text{Accuracy}_{\\text{all}\\setminus f_i}$.  \n",
        "- Equivalent to leave-one-feature-out importance.\n",
        "\n",
        "Cheaper when kernel already available; avoids entropy estimation.\n",
        "\n",
        "---\n",
        "\n",
        "## 5) Hybrid quantum-classical selection loop  \n",
        "\n",
        "```python\n",
        "features = list(range(p))          # p original features\n",
        "scores = []\n",
        "for i in features:\n",
        "    score = quantum_MI(feature=i, data=X, labels=y, shots=2048)\n",
        "    scores.append((i, score))\n",
        "top_k = [i for i,_ in sorted(scores, key=lambda t:-t[1])[:k]]\n",
        "\n",
        "# downstream model\n",
        "X_reduced = X[:, top_k]\n",
        "qkernel = QuantumKernel(feature_map=ZZFeatureMap(len(top_k)), quantum_instance=backend)\n",
        "clf = SVC(kernel=qkernel.evaluate).fit(X_reduced, y)\n",
        "```\n",
        "\n",
        "Use classical RFE or LASSO in place of MI if desired; quantum part supplies kernel.\n",
        "\n",
        "---\n",
        "\n",
        "## 6) Qiskit experiment — breast-cancer dataset (mini demo)\n",
        "\n",
        "```python\n",
        "from qiskit_machine_learning.datasets import breast_cancer\n",
        "from qiskit_machine_learning.kernels import QuantumKernel\n",
        "from qiskit.circuit.library import ZZFeatureMap\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "X, y = breast_cancer(training_size=100, test_size=50, n=6, plot_data=False)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# feature relevance via simple variance as placeholder (replace with quantum_MI)\n",
        "variances = np.var(X_train, axis=0)\n",
        "top_idx = np.argsort(variances)[-3:]      # keep 3 features\n",
        "\n",
        "feature_map = ZZFeatureMap(num_qubits=len(top_idx), reps=1)\n",
        "qkernel = QuantumKernel(feature_map=feature_map, quantum_instance=BasicAer.get_backend('statevector_simulator'))\n",
        "clf = SVC(kernel=qkernel.evaluate).fit(X_train[:, top_idx], y_train)\n",
        "print(\"QSVM acc:\", clf.score(X_test[:, top_idx], y_test))\n",
        "```\n",
        "\n",
        "Replace variance scoring with QMI routine for homework.\n",
        "\n",
        "---\n",
        "\n",
        "## 7) Noise & mitigation  \n",
        "\n",
        "| Noise source | Effect on MI / kernel | Counter-measure |\n",
        "|--------------|-----------------------|-----------------|\n",
        "| SPAM errors  | Bias probabilities    | Read-out calibration |\n",
        "| Decoherence  | Shrinks off-diagonal  | Dynamical decoupling |\n",
        "| Shot noise   | Variance in scores    | Adaptive shot allocation |\n",
        "| Gate errors  | Systematic bias       | Zero-noise extrapolation (ZNE), SKQD |\n",
        "\n",
        "**Emerging**: *SQD/SKQD* incorporate stochastic noise models directly into MI estimation to debias scores.\n",
        "\n",
        "---\n",
        "\n",
        "## 8) Mini-exercises (answers in Appendix)\n",
        "\n",
        "1. Derive swap-test probability $p_\\text{swap}=\\tfrac12+\\tfrac12\\operatorname{Tr}(\\rho^2)$.  \n",
        "2. For 4 features, how many qubits needed to estimate pairwise QMI between each feature and 1-qubit label using swap tests?  \n",
        "3. Implement kernel leave-one-feature-out score and show accuracy change on iris binary dataset for $k=2$ vs all features.  \n",
        "4. Under depolarising error $p=0.02$ per two-qubit gate, estimate bias introduced in QMI of single qubit (hint: fidelity shrink factor).  \n",
        "5. Explain why QMI may detect non-linear dependencies missed by Pearson correlation.\n",
        "\n",
        "---\n",
        "\n",
        "## 9) FAQ  \n",
        "\n",
        "- **“Can I encode >1 feature per qubit?”** Yes via data-reuploading circuits; selection then chooses which parameters, not qubits.  \n",
        "- **“Do I need two copies of the state for swap test?”** Yes; alternative: classical shadows to estimate purity with fewer qubits.  \n",
        "- **“Is quantum MI always better than classical?”** No—advantage is conjectured for data with complex entangled structure or when classical MI estimation is high-dimensional.  \n",
        "- **“How large a dataset can I handle?”** Kernel matrix still $O(N^2)$ circuits; feature selection reduces qubits but not sample scaling.\n",
        "\n",
        "---\n",
        "\n",
        "## 10) Summary (Session 11)\n",
        "\n",
        "- Feature selection is key to bringing high-dimensional data onto today’s small quantum processors.  \n",
        "- **Quantum mutual information** and **kernel-based importance** provide principled relevance scores.  \n",
        "- Hybrid paradigm: quantum scoring, classical ranking, quantum or classical downstream model.  \n",
        "- Qiskit Machine Learning offers tools (`QuantumKernel`, feature maps) to prototype quickly; swap-test and shadows needed for MI experiments.  \n",
        "- Noise remains main bottleneck; mitigation plus emerging SQD/SKQD techniques show promise.\n",
        "\n",
        "---\n",
        "\n",
        "## 11) Looking ahead  \n",
        "\n",
        "- **Next Session:** Mid-term review and open project brainstorming.  \n",
        "- **Homework 3 add-on:**  \n",
        "  - Implement QMI feature ranking on small synthetic data; compare to mutual information in `scikit-learn`.  \n",
        "  - Evaluate QSVM accuracy vs #features (1–4) under shot noise 1024.\n",
        "\n",
        "---\n",
        "\n",
        "## Appendix — mini-exercise solutions (sketch)\n",
        "\n",
        "1. Swap-test derivation: $\\Pr(|0\\rangle)=\\frac12+\\frac12\\operatorname{Tr}(\\rho\\sigma)$; set $\\sigma=\\rho$.  \n",
        "2. Need 2 copies per swap test → 2*(feature+label)=10 qubits including ancilla.  \n",
        "3. Accuracy drops from 0.97 (all) to 0.95 (top-2 features).  \n",
        "4. Purity scales as $(1-4p/3)^{g}$ where $g$=gate count; bias ≈ 0.92 for 3 entangling gates.  \n",
        "5. Pearson captures linear relation; QMI detects any probabilistic dependence including XOR-type parity.\n",
        "\n"
      ],
      "metadata": {
        "id": "jGckbqdARP07"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DzRpdOT7SeNI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}